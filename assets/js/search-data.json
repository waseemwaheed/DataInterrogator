{
  
    
        "post0": {
            "title": "Efficient Data Analysis - SQL and Python",
            "content": "Introduction . Performing data analysis, in many cases, requires loading the data from a database. Database engines are optimized for the efficient handling of data storage and retrieval. If the data is structured, which means the data follows a schema, querying the database is always performed using SQL, a domain specific language for data querying among other things. . I have been using Jupyter notebooks for 4 years now, and I think the notebook concept is transformational. Being able to mix the analysis with the compute in one linear document is great. The other day I was looking at some SQL queries and thought to myself, I wish that could write those queries in Jupyter notebooks just like I do with Python, I had previously tried Matlab and Julia, why not SQL? . As you would imagine, the community has thought about this question way before I did and someone has made that possible in the form of a IPython extension üòç. . This post is my attempt at using SQL in the Jupyter notebook environment. The data we are going to use in this excercise the 20 years of Olympic history: athletes and results from Kaggle. The analysis presented here is for practice only and for more detailed analysis of the olympic games, check Olympians are probably older ‚Äî and younger ‚Äî than you think. . Motivation . I have few reasons for writing this article: . As a reminder for my future self, cause I tend to move on and forget üòÖ as you can imagine, | I hope it serves as an inspiration to you, the reader, to utilize this capability if you like it, | Practice SQL, which I can&#39;t have enough of, | Being able to send the calculations to the data rather than bring the data to compute is a super power in the Big Data era, | Bringing the result of SQL queries back to Python rather than querying the database directly or using something like SQL Server Management Studio or MySQL Workbench allows us to visualize the data in whatever way we like. | It is important to mention that whatever we are doing here can be done within Pandas directly, however from a scalability and effeciency perspectives, the ability to do most of the filtering and summerization at the database level is a clear advantage. . The database can be hosted anywhere. Which means the machine on which the analysis is carried out doesn&#39;t need to be highly resourced. . Why not an ORM? . ORM stands for Object Relational Mapping which according to wikipedia: . a programming technique for converting data between incompatible type systems using object-oriented programming languages. This creates, in effect, a &quot;virtual object database&quot; that can be used from within the programming language. . ORMs hide the typical SQL interaction from the software developer, exposing the database as classes and objects with getters and setters. An typical example of an ORM are SQLAlchemy and SQLModel which builds on top SQLAlchemy. If you think that this is a good idea, I tend to agree as do most web frameworks but, the goal of this article is not to run away from SQL but to embrace it as universal tool that can be utilized on its own and in combination with most programming languages. . Why not Spark (or the likes) . This is a great question. If you have heard of Big Data tools such as Spark, this question would defintely come to your mind. Spark does what we are trying to achieve, and I am planning to cover it in a follow up writeup. . Hopefully, I managed to convince you of the benefits of being able to query databases using SQL rather than doing the same analysis in Pandas. . Required packages . I am going to use the following packages in this article: . ipython-sql | Pandas | Matplotlib | SQLAlchemy | . Connecting to database . This first step is to establish a connection to the database you would like to work on. We are going to use the ipython-sql extension to write and pass our SQL queries. ipython-sql expects database string similar to those used by SQLAlcehmy. For more details about the database strings check this page. . In this article, I am going to use a local installation of Microsoft SQL Server 2019 Express. I have already downloaded MS-SQL server already and installed it. I then created a new database named OlympicsHistory and imported the two csv files from the Olympic History dataset into this database as a tables named athlete_events, noc_region . . %load_ext sql import pandas as pd import matplotlib.pyplot as plt . %sql mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?trusted_connection=yes&amp;driver=SQL+Server . How to handle queries and results . Now that you succeeded connecting to the database, let&#39;s see how can we bring data from the database into Python. The ipython-sql extension offers three mode of access: . 1. Print the results 2. Assignning the result of a single-line query 3. Assignning the result of a multi-line query . Note: ignore the details of the queries for now and let&#39;s focus on the handling of queries, we will return to the details of the queries in the following section. . 1. Print the results . You use the magic command (to learn more) %sql for a single line and %%sql for multi-line queries. . 1.1. Single line query . %sql SELECT DISTINCT TOP 3 Sport FROM athlete_events . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . Sport . Basketball | . Judo | . Football | . 1.2. Multi-line query . %%sql SELECT DISTINCT TOP 3 Sport FROM athlete_events . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . Sport . Basketball | . Judo | . Football | . 2. Assignning the result of a single-line query . result = %sql SELECT DISTINCT TOP 3 Sport FROM athlete_events print(result) . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. ++ | Sport | ++ | Basketball | | Judo | | Football | ++ . %%sql result &lt;&lt; SELECT TOP 3 year, Count(CASE WHEN Medal = &#39;Bronze&#39; THEN 1 END) Bronze, Count(CASE WHEN Medal = &#39;Silver&#39; THEN 1 END) Silver, Count(CASE WHEN Medal = &#39;Gold&#39; THEN 1 END) Gold FROM athlete_events GROUP BY Year ORDER BY Year . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. Returning data to local variable result . print(result) . ++--+--++ | year | Bronze | Silver | Gold | ++--+--++ | 1896 | 38 | 43 | 62 | | 1900 | 175 | 228 | 201 | | 1904 | 150 | 163 | 173 | ++--+--++ . If have used Python for data analysis, you might say this cool but, it would even nice if we could capture the result of the query as Pandas DataFrame! If you had this question in mind, to you I say, it is straight forward to that, have a look: . df = result.DataFrame() print(df) . year Bronze Silver Gold 0 1896 38 43 62 1 1900 175 228 201 2 1904 150 163 173 . type(df) . pandas.core.frame.DataFrame . df.describe() . year Bronze Silver Gold . count 3.0 | 3.000000 | 3.000000 | 3.000000 | . mean 1900.0 | 121.000000 | 144.666667 | 145.333333 | . std 4.0 | 72.958893 | 93.852722 | 73.514171 | . min 1896.0 | 38.000000 | 43.000000 | 62.000000 | . 25% 1898.0 | 94.000000 | 103.000000 | 117.500000 | . 50% 1900.0 | 150.000000 | 163.000000 | 173.000000 | . 75% 1902.0 | 162.500000 | 195.500000 | 187.000000 | . max 1904.0 | 175.000000 | 228.000000 | 201.000000 | . I hope that you can see the power in what I just demonstrated. . Data Analysis using SQL . Enough of the preparation, let&#39;s into the fun part, analysing the data, asking and answering questions about it. . As a first step, let&#39;s familiarize ourselves with the two tables we have . %sql SELECT TOP 5 * FROM athlete_events . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . ID Name Sex Age Height Weight Team NOC Games Year Season City Sport Event Medal . 1 | A Dijiang | M | 24 | 180 | 80 | China | CHN | 1992 Summer | 1992 | Summer | Barcelona | Basketball | Basketball Men&amp;#x27;s Basketball | NA | . 2 | A Lamusi | M | 23 | 170 | 60 | China | CHN | 2012 Summer | 2012 | Summer | London | Judo | Judo Men&amp;#x27;s Extra-Lightweight | NA | . 3 | Gunnar Nielsen Aaby | M | 24 | NA | NA | Denmark | DEN | 1920 Summer | 1920 | Summer | Antwerpen | Football | Football Men&amp;#x27;s Football | NA | . 4 | Edgar Lindenau Aabye | M | 34 | NA | NA | Denmark/Sweden | DEN | 1900 Summer | 1900 | Summer | Paris | Tug-Of-War | Tug-Of-War Men&amp;#x27;s Tug-Of-War | Gold | . 5 | Christine Jacoba Aaftink | F | 21 | 185 | 82 | Netherlands | NED | 1988 Winter | 1988 | Winter | Calgary | Speed Skating | Speed Skating Women&amp;#x27;s 500 metres | NA | . %sql SELECT TOP 5 * FROM noc_regions . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . NOC region notes . AFG | Afghanistan | None | . AHO | Curacao | Netherlands Antilles | . ALB | Albania | None | . ALG | Algeria | None | . AND | Andorra | None | . %%sql SELECT DISTINCT athlete_events.noc FROM athlete_events LEFT JOIN noc_regions ON athlete_events.noc = noc_regions.noc WHERE region IS NULL . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . noc . SGP | . Looking up SGP reveals that this is the code for Singapore. This begs the question, why was this code missing from the noc_regions table? Let&#39;s further investigate this table . %%sql SELECT * FROM noc_regions WHERE region LIKE &#39;%pore&#39; . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . NOC region notes . SIN | Singapore | None | . Let&#39;s further investigate the athlete_events table for instances of the codes SIN and SGP . %%sql SELECT DISTINCT team, noc FROM athlete_events WHERE team LIKE &#39;%pore&#39; OR noc = &#39;SGP&#39; OR noc = &#39;SIN&#39; . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . team noc . June Climene | SGP | . Rika II | SGP | . Singapore | SGP | . Singapore-1 | SGP | . Singapore-2 | SGP | . Ok, so the code SIN has never been used in the athlete_events. A quick web search revealed that SGP has replaced SIN in 2016. . In this case, I think, it would be a good idea to keep both codes in the noc_regions table. Let&#39;s add SGP . %%sql IF NOT EXISTS (SELECT * FROM noc_regions WHERE noc = &#39;SGP&#39;) INSERT INTO noc_regions VALUES (&#39;SGP&#39;, &#39;Singapore&#39;, &#39;Added by Waseem&#39;) . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes 1 rows affected. . ResourceClosedError Traceback (most recent call last) &lt;ipython-input-16-a053653f5b5d&gt; in &lt;module&gt; -&gt; 1 get_ipython().run_cell_magic(&#39;sql&#39;, &#39;&#39;, &#34;IF NOT EXISTS (SELECT * n FROM noc_regions n WHERE noc = &#39;SGP&#39;) n INSERT INTO noc_regions n VALUES (&#39;SGP&#39;, n &#39;Singapore&#39;, n &#39;Added by Waseem&#39;) n&#34;) ~ anaconda3 lib site-packages IPython core interactiveshell.py in run_cell_magic(self, magic_name, line, cell) 2397 with self.builtin_trap: 2398 args = (magic_arg_s, cell) -&gt; 2399 result = fn(*args, **kwargs) 2400 return result 2401 ~ anaconda3 lib site-packages decorator.py in fun(*args, **kw) 229 if not kwsyntax: 230 args, kw = fix(args, kw, sig) --&gt; 231 return caller(func, *(extras + args), **kw) 232 fun.__name__ = func.__name__ 233 fun.__doc__ = func.__doc__ ~ anaconda3 lib site-packages IPython core magic.py in &lt;lambda&gt;(f, *a, **k) 185 # but it&#39;s overkill for just that one bit of state. 186 def magic_deco(arg): --&gt; 187 call = lambda f, *a, **k: f(*a, **k) 188 189 if callable(arg): ~ anaconda3 lib site-packages decorator.py in fun(*args, **kw) 229 if not kwsyntax: 230 args, kw = fix(args, kw, sig) --&gt; 231 return caller(func, *(extras + args), **kw) 232 fun.__name__ = func.__name__ 233 fun.__doc__ = func.__doc__ ~ anaconda3 lib site-packages IPython core magic.py in &lt;lambda&gt;(f, *a, **k) 185 # but it&#39;s overkill for just that one bit of state. 186 def magic_deco(arg): --&gt; 187 call = lambda f, *a, **k: f(*a, **k) 188 189 if callable(arg): ~ anaconda3 lib site-packages sql magic.py in execute(self, line, cell, local_ns) 215 216 try: --&gt; 217 result = sql.run.run(conn, parsed[&#34;sql&#34;], self, user_ns) 218 219 if ( ~ anaconda3 lib site-packages sql run.py in run(conn, sql, config, user_namespace) 369 if result and config.feedback: 370 print(interpret_rowcount(result.rowcount)) --&gt; 371 resultset = ResultSet(result, statement, config) 372 if config.autopandas: 373 return resultset.DataFrame() ~ anaconda3 lib site-packages sql run.py in __init__(self, sqlaproxy, sql, config) 105 106 def __init__(self, sqlaproxy, sql, config): --&gt; 107 self.keys = sqlaproxy.keys() 108 self.sql = sql 109 self.config = config ~ anaconda3 lib site-packages sqlalchemy engine result.py in keys(self) 705 706 &#34;&#34;&#34; --&gt; 707 return self._metadata.keys 708 709 ~ anaconda3 lib site-packages sqlalchemy engine cursor.py in keys(self) 1199 @property 1200 def keys(self): -&gt; 1201 self._we_dont_return_rows() 1202 1203 ~ anaconda3 lib site-packages sqlalchemy engine cursor.py in _we_dont_return_rows(self, err) 1176 1177 def _we_dont_return_rows(self, err=None): -&gt; 1178 util.raise_( 1179 exc.ResourceClosedError( 1180 &#34;This result object does not return rows. &#34; ~ anaconda3 lib site-packages sqlalchemy util compat.py in raise_(***failed resolving arguments***) 209 210 try: --&gt; 211 raise exception 212 finally: 213 # credit to ResourceClosedError: This result object does not return rows. It has been closed automatically. . . There seems to be a bug in SQLAlchemy which makes ealier SQL code error. We can safely ignore this error as the code seems to be doing what is supposed to do, let&#39;s verify: . %%sql SELECT * FROM noc_regions WHERE noc = &#39;SGP&#39; . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . NOC region notes . SGP | Singapore | Added by Waseem | . We can now perform joins without any NULLs. . Let&#39;s begin with the data interrogation: . Which cities host the Olympics more that once? . %%sql cities &lt;&lt; SELECT City, Count(Year) AS NumTimes FROM (SELECT DISTINCT year, city FROM athlete_events) city GROUP BY city HAVING Count(year) &gt; 1 ORDER BY numtimes DESC . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. Returning data to local variable cities . cities_df = cities.DataFrame() cities_df.plot.bar(x=&#39;City&#39;, y=&#39;NumTimes&#39;) . &lt;AxesSubplot:xlabel=&#39;City&#39;&gt; . %%sql CREATE OR ALTER VIEW city_year AS SELECT DISTINCT City, Year FROM athlete_events . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . ResourceClosedError Traceback (most recent call last) &lt;ipython-input-20-4d4e806a16fc&gt; in &lt;module&gt; -&gt; 1 get_ipython().run_cell_magic(&#39;sql&#39;, &#39;&#39;, &#39;CREATE OR ALTER VIEW city_year AS nSELECT DISTINCT City, n Year nFROM athlete_events n&#39;) ~ anaconda3 lib site-packages IPython core interactiveshell.py in run_cell_magic(self, magic_name, line, cell) 2397 with self.builtin_trap: 2398 args = (magic_arg_s, cell) -&gt; 2399 result = fn(*args, **kwargs) 2400 return result 2401 ~ anaconda3 lib site-packages decorator.py in fun(*args, **kw) 229 if not kwsyntax: 230 args, kw = fix(args, kw, sig) --&gt; 231 return caller(func, *(extras + args), **kw) 232 fun.__name__ = func.__name__ 233 fun.__doc__ = func.__doc__ ~ anaconda3 lib site-packages IPython core magic.py in &lt;lambda&gt;(f, *a, **k) 185 # but it&#39;s overkill for just that one bit of state. 186 def magic_deco(arg): --&gt; 187 call = lambda f, *a, **k: f(*a, **k) 188 189 if callable(arg): ~ anaconda3 lib site-packages decorator.py in fun(*args, **kw) 229 if not kwsyntax: 230 args, kw = fix(args, kw, sig) --&gt; 231 return caller(func, *(extras + args), **kw) 232 fun.__name__ = func.__name__ 233 fun.__doc__ = func.__doc__ ~ anaconda3 lib site-packages IPython core magic.py in &lt;lambda&gt;(f, *a, **k) 185 # but it&#39;s overkill for just that one bit of state. 186 def magic_deco(arg): --&gt; 187 call = lambda f, *a, **k: f(*a, **k) 188 189 if callable(arg): ~ anaconda3 lib site-packages sql magic.py in execute(self, line, cell, local_ns) 215 216 try: --&gt; 217 result = sql.run.run(conn, parsed[&#34;sql&#34;], self, user_ns) 218 219 if ( ~ anaconda3 lib site-packages sql run.py in run(conn, sql, config, user_namespace) 369 if result and config.feedback: 370 print(interpret_rowcount(result.rowcount)) --&gt; 371 resultset = ResultSet(result, statement, config) 372 if config.autopandas: 373 return resultset.DataFrame() ~ anaconda3 lib site-packages sql run.py in __init__(self, sqlaproxy, sql, config) 105 106 def __init__(self, sqlaproxy, sql, config): --&gt; 107 self.keys = sqlaproxy.keys() 108 self.sql = sql 109 self.config = config ~ anaconda3 lib site-packages sqlalchemy engine result.py in keys(self) 705 706 &#34;&#34;&#34; --&gt; 707 return self._metadata.keys 708 709 ~ anaconda3 lib site-packages sqlalchemy engine cursor.py in keys(self) 1199 @property 1200 def keys(self): -&gt; 1201 self._we_dont_return_rows() 1202 1203 ~ anaconda3 lib site-packages sqlalchemy engine cursor.py in _we_dont_return_rows(self, err) 1176 1177 def _we_dont_return_rows(self, err=None): -&gt; 1178 util.raise_( 1179 exc.ResourceClosedError( 1180 &#34;This result object does not return rows. &#34; ~ anaconda3 lib site-packages sqlalchemy util compat.py in raise_(***failed resolving arguments***) 209 210 try: --&gt; 211 raise exception 212 finally: 213 # credit to ResourceClosedError: This result object does not return rows. It has been closed automatically. . . %%sql SELECT City, Count(Year) AS NumTimes FROM city_year GROUP BY City HAVING Count(Year) &gt; 1 ORDER BY numtimes DESC . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . City NumTimes . London | 3 | . Athina | 3 | . Sankt Moritz | 2 | . Innsbruck | 2 | . Lake Placid | 2 | . Stockholm | 2 | . Los Angeles | 2 | . Paris | 2 | . Find the seasons for each year as two columns (Summer, Winter) . %%sql SELECT * FROM (SELECT DISTINCT PARSENAME(REPLACE(Games, &#39; &#39;, &#39;.&#39;), 2) AS Year, PARSENAME(REPLACE(Games, &#39; &#39;, &#39;.&#39;), 1) AS Season FROM athlete_events) t1 PIVOT ( Count(Season) FOR Season IN (Summer, Winter) ) piv ORDER BY Year . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . Year Summer Winter . 1896 | 1 | 0 | . 1900 | 1 | 0 | . 1904 | 1 | 0 | . 1906 | 1 | 0 | . 1908 | 1 | 0 | . 1912 | 1 | 0 | . 1920 | 1 | 0 | . 1924 | 1 | 1 | . 1928 | 1 | 1 | . 1932 | 1 | 1 | . 1936 | 1 | 1 | . 1948 | 1 | 1 | . 1952 | 1 | 1 | . 1956 | 1 | 1 | . 1960 | 1 | 1 | . 1964 | 1 | 1 | . 1968 | 1 | 1 | . 1972 | 1 | 1 | . 1976 | 1 | 1 | . 1980 | 1 | 1 | . 1984 | 1 | 1 | . 1988 | 1 | 1 | . 1992 | 1 | 1 | . 1994 | 0 | 1 | . 1996 | 1 | 0 | . 1998 | 0 | 1 | . 2000 | 1 | 0 | . 2002 | 0 | 1 | . 2004 | 1 | 0 | . 2006 | 0 | 1 | . 2008 | 1 | 0 | . 2010 | 0 | 1 | . 2012 | 1 | 0 | . 2014 | 0 | 1 | . 2016 | 1 | 0 | . What is the average age of participants? . %%sql SELECT Avg(Cast(Age AS INT)) as AvgAge FROM athlete_events WHERE age IS NOT NULL AND age &lt;&gt; &#39;NA&#39; . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . AvgAge . 25 | . What is the average age per season? . %%sql SELECT Season, Avg(CAST(Age AS INT)) AvgAge FROM athlete_events WHERE Age &lt;&gt; &#39;NA&#39; GROUP BY Season . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . Season AvgAge . Summer | 25 | . Winter | 25 | . Is the average age fixed across the seasons and years? . %%sql result &lt;&lt; SELECT Year, Season, AVG(cast(Age as int)) AverageAge FROM athlete_events WHERE Age is not NULL AND Age &lt;&gt; &#39;NA&#39; GROUP BY Year, Season ORDER BY Year, Season . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. Returning data to local variable result . result_df = result.DataFrame() summer_df = result_df[result_df[&#39;Season&#39;] == &#39;Summer&#39;] winter_df = result_df[result_df[&#39;Season&#39;] == &#39;Winter&#39;] . summer_df.plot.scatter(x=&#39;Year&#39;,y=&#39;AverageAge&#39;, title=&#39;Summer Participants Average Age&#39;) winter_df.plot.scatter(x=&#39;Year&#39;,y=&#39;AverageAge&#39;, title=&#39;Winter Participants Average Age&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Winter Participants Average Age&#39;}, xlabel=&#39;Year&#39;, ylabel=&#39;AverageAge&#39;&gt; . The average for summer Olympics in 1932 looks too high, is this a valid result? if yes why did it happen? . %%sql select AVG(cast(Age as int)) as AvgAge from athlete_events left join noc_regions on athlete_events.NOC = noc_regions.NOC where Year=1932 and Age &lt;&gt; &#39;NA&#39; and Season=&#39;Summer&#39; . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . AvgAge . 33 | . So, the average age was high, why is that the case? Let&#39;s look at a more statistics about the games broken down by year and season. . %%sql SELECT year, Min(CASE WHEN season = &#39;Summer&#39; THEN Cast(age AS INT) END) SummerMinAge, Max(CASE WHEN season = &#39;Summer&#39; THEN Cast(age AS INT) END) SummerMaxAge, Avg(CASE WHEN season = &#39;Summer&#39; THEN Cast(age AS INT) END) SummerAvgAge, Var(CASE WHEN season = &#39;Summer&#39; THEN Cast(age AS INT) END) SummerVarAge, Min(CASE WHEN season = &#39;Winter&#39; THEN Cast(age AS INT) END) WinterMinAge, Max(CASE WHEN season = &#39;Winter&#39; THEN Cast(age AS INT) END) WinterMaxAge, Avg(CASE WHEN season = &#39;Winter&#39; THEN Cast(age AS INT) END) WinterAvgAge, Var(CASE WHEN season = &#39;Winter&#39; THEN Cast(age AS INT) END) WinterVarAge FROM athlete_events WHERE age IS NOT NULL AND age &lt;&gt; &#39;NA&#39; GROUP BY year ORDER BY year . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . year SummerMinAge SummerMaxAge SummerAvgAge SummerVarAge WinterMinAge WinterMaxAge WinterAvgAge WinterVarAge . 1896 | 10 | 40 | 23 | 22.02240143369178 | None | None | None | None | . 1900 | 13 | 71 | 29 | 87.57875351516955 | None | None | None | None | . 1904 | 14 | 71 | 26 | 76.6066534940619 | None | None | None | None | . 1906 | 13 | 54 | 27 | 62.617258530706444 | None | None | None | None | . 1908 | 14 | 61 | 26 | 61.15578403594632 | None | None | None | None | . 1912 | 13 | 67 | 27 | 64.84038295212197 | None | None | None | None | . 1920 | 13 | 72 | 29 | 68.44374620002326 | None | None | None | None | . 1924 | 13 | 81 | 28 | 74.08304035936791 | 11 | 58 | 27 | 48.97374171326985 | . 1928 | 11 | 97 | 29 | 118.93684427853675 | 15 | 54 | 26 | 37.10742966900141 | . 1932 | 13 | 96 | 33 | 202.85578680390805 | 11 | 52 | 25 | 33.449755356216194 | . 1936 | 12 | 74 | 27 | 76.48352848554916 | 11 | 46 | 25 | 24.044063840363208 | . 1948 | 12 | 84 | 29 | 93.24748654592041 | 15 | 53 | 26 | 32.50355943000253 | . 1952 | 13 | 65 | 26 | 43.83151104313369 | 12 | 47 | 25 | 26.63744994317875 | . 1956 | 13 | 67 | 26 | 43.08506131207035 | 12 | 48 | 25 | 25.929199837782704 | . 1960 | 12 | 65 | 25 | 38.08302692628261 | 11 | 39 | 24 | 20.080557267666588 | . 1964 | 12 | 60 | 25 | 31.975105597154663 | 13 | 53 | 24 | 21.768654166104547 | . 1968 | 11 | 68 | 24 | 36.317311148018874 | 11 | 51 | 24 | 18.71876641678888 | . 1972 | 12 | 69 | 24 | 35.930160546839694 | 13 | 42 | 24 | 19.98596052286161 | . 1976 | 12 | 70 | 23 | 32.802134361215664 | 12 | 46 | 23 | 21.7698320494643 | . 1980 | 13 | 70 | 23 | 27.768520445745068 | 13 | 49 | 23 | 18.154642359559396 | . 1984 | 12 | 60 | 24 | 30.05881562619138 | 15 | 53 | 23 | 16.397695807730326 | . 1988 | 13 | 70 | 24 | 29.150256275905534 | 11 | 52 | 23 | 17.46205581946204 | . 1992 | 11 | 62 | 24 | 29.238076996153065 | 13 | 46 | 24 | 17.08599797046234 | . 1994 | None | None | None | None | 13 | 46 | 24 | 17.603527791814635 | . 1996 | 12 | 63 | 24 | 30.263060247773502 | None | None | None | None | . 1998 | None | None | None | None | 14 | 50 | 25 | 19.702398658160618 | . 2000 | 13 | 63 | 25 | 29.60243153446837 | None | None | None | None | . 2002 | None | None | None | None | 15 | 48 | 25 | 22.23690379230238 | . 2004 | 13 | 57 | 25 | 31.13741181652509 | None | None | None | None | . 2006 | None | None | None | None | 14 | 52 | 25 | 23.876212746402683 | . 2008 | 12 | 67 | 25 | 32.32947785953114 | None | None | None | None | . 2010 | None | None | None | None | 15 | 51 | 26 | 25.120888173261157 | . 2012 | 13 | 71 | 25 | 32.28653359213402 | None | None | None | None | . 2014 | None | None | None | None | 15 | 55 | 25 | 23.517221690522096 | . 2016 | 13 | 62 | 26 | 30.917678201428895 | None | None | None | None | . It is clear that there&#39;s a high variability in participants ages in 1932. Let&#39;s have a closer look at the data to find out what could the reason behind that be. . In which sport did older participants take part in? . %%sql select top 10 * from athlete_events where year = &#39;1932&#39; and season = &#39;Summer&#39; and age &lt;&gt; &#39;NA&#39; and age = 96 . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . ID Name Sex Age Height Weight Team NOC Games Year Season City Sport Event Medal . 49663 | Winslow Homer | M | 96 | NA | NA | United States | USA | 1932 Summer | 1932 | Summer | Los Angeles | Art Competitions | Art Competitions Mixed Painting, Unknown Event | NA | . Oh, the older folks participated in Art Competitions. Let&#39;s see what impact does removing Art Competitions have on the age distribution. . %%sql SELECT year, Min(CASE WHEN season = &#39;Summer&#39; THEN Cast(age AS INT) END) SummerMinAge, Max(CASE WHEN season = &#39;Summer&#39; THEN Cast(age AS INT) END) SummerMaxAge, Avg(CASE WHEN season = &#39;Summer&#39; THEN Cast(age AS INT) END) SummerAvgAge, Var(CASE WHEN season = &#39;Summer&#39; THEN Cast(age AS INT) END) SummerVarAge, Min(CASE WHEN season = &#39;Winter&#39; THEN Cast(age AS INT) END) WinterMinAge, Max(CASE WHEN season = &#39;Winter&#39; THEN Cast(age AS INT) END) WinterMaxAge, Avg(CASE WHEN season = &#39;Winter&#39; THEN Cast(age AS INT) END) WinterAvgAge, Var(CASE WHEN season = &#39;Winter&#39; THEN Cast(age AS INT) END) WinterVarAge FROM athlete_events WHERE age IS NOT NULL AND age &lt;&gt; &#39;NA&#39; AND Sport &lt;&gt; &#39;Art Competitions&#39; GROUP BY year ORDER BY year . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . year SummerMinAge SummerMaxAge SummerAvgAge SummerVarAge WinterMinAge WinterMaxAge WinterAvgAge WinterVarAge . 1896 | 10 | 40 | 23 | 22.02240143369178 | None | None | None | None | . 1900 | 13 | 71 | 29 | 87.57875351516955 | None | None | None | None | . 1904 | 14 | 71 | 26 | 76.6066534940619 | None | None | None | None | . 1906 | 13 | 54 | 27 | 62.617258530706444 | None | None | None | None | . 1908 | 14 | 61 | 26 | 61.15578403594632 | None | None | None | None | . 1912 | 13 | 64 | 27 | 62.604400100967034 | None | None | None | None | . 1920 | 13 | 72 | 29 | 67.98906348229981 | None | None | None | None | . 1924 | 13 | 60 | 27 | 54.157015903460966 | 11 | 58 | 27 | 48.97374171326985 | . 1928 | 11 | 63 | 25 | 47.05709575019104 | 15 | 54 | 26 | 37.10742966900141 | . 1932 | 13 | 54 | 25 | 38.055043191309956 | 11 | 52 | 25 | 33.449755356216194 | . 1936 | 12 | 72 | 25 | 35.960182685284465 | 11 | 46 | 25 | 24.044063840363208 | . 1948 | 12 | 61 | 27 | 50.99427202894765 | 15 | 53 | 26 | 32.50355943000253 | . 1952 | 13 | 65 | 26 | 43.83151104313369 | 12 | 47 | 25 | 26.63744994317875 | . 1956 | 13 | 67 | 26 | 43.08506131207035 | 12 | 48 | 25 | 25.929199837782704 | . 1960 | 12 | 65 | 25 | 38.08302692628261 | 11 | 39 | 24 | 20.080557267666588 | . 1964 | 12 | 60 | 25 | 31.975105597154663 | 13 | 53 | 24 | 21.768654166104547 | . 1968 | 11 | 68 | 24 | 36.317311148018874 | 11 | 51 | 24 | 18.71876641678888 | . 1972 | 12 | 69 | 24 | 35.930160546839694 | 13 | 42 | 24 | 19.98596052286161 | . 1976 | 12 | 70 | 23 | 32.802134361215664 | 12 | 46 | 23 | 21.7698320494643 | . 1980 | 13 | 70 | 23 | 27.768520445745068 | 13 | 49 | 23 | 18.154642359559396 | . 1984 | 12 | 60 | 24 | 30.05881562619138 | 15 | 53 | 23 | 16.397695807730326 | . 1988 | 13 | 70 | 24 | 29.150256275905534 | 11 | 52 | 23 | 17.46205581946204 | . 1992 | 11 | 62 | 24 | 29.238076996153065 | 13 | 46 | 24 | 17.08599797046234 | . 1994 | None | None | None | None | 13 | 46 | 24 | 17.603527791814635 | . 1996 | 12 | 63 | 24 | 30.263060247773502 | None | None | None | None | . 1998 | None | None | None | None | 14 | 50 | 25 | 19.702398658160618 | . 2000 | 13 | 63 | 25 | 29.60243153446837 | None | None | None | None | . 2002 | None | None | None | None | 15 | 48 | 25 | 22.23690379230238 | . 2004 | 13 | 57 | 25 | 31.13741181652509 | None | None | None | None | . 2006 | None | None | None | None | 14 | 52 | 25 | 23.876212746402683 | . 2008 | 12 | 67 | 25 | 32.32947785953114 | None | None | None | None | . 2010 | None | None | None | None | 15 | 51 | 26 | 25.120888173261157 | . 2012 | 13 | 71 | 25 | 32.28653359213402 | None | None | None | None | . 2014 | None | None | None | None | 15 | 55 | 25 | 23.517221690522096 | . 2016 | 13 | 62 | 26 | 30.917678201428895 | None | None | None | None | . I am statisfied with this now, it seems to match the result reported in here. . What is trend of age in each season? . This is a fun question, let&#39;s try to find the slope of the best fit line across the previous plots. This can be achieved by implementing the equation of the slope of single variable linear regression: $$b= frac{n sum{xy} - sum{x} sum{y}}{n sum{x^2}-( sum{x})^2}$$ . %%sql select (n*SigmaXY - SigmaX * SigmaY)/(n*SigmaX2 - SigmaX^2) as b from ( select cast(Sum(Y) as float) as SigmaY, sum(X^2) as SigmaX2, sum(X) as SigmaX, sum(Y*X) as SigmaXY, count(*) as n from ( SELECT Year-1896 as X, AVG(cast(Age as int)) as Y FROM athlete_events WHERE Age is not NULL AND Age &lt;&gt; &#39;NA&#39; AND Sport &lt;&gt; &#39;Art Competitions&#39; AND Season = &#39;Summer&#39; GROUP BY Year ) t1 ) t2 . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . b . -0.4668425612349374 | . %%sql SELECT ( n * sigmaxy - sigmax * sigmay ) / ( n * sigmax2 - sigmax^2 ) AS b FROM (SELECT Cast(Sum(y) AS FLOAT) AS SigmaY, Sum(x^2) AS SigmaX2, Sum(x) AS SigmaX, Sum(y * x) AS SigmaXY, Count(*) AS n FROM (SELECT year - 1896 AS X, Avg(Cast(age AS INT)) AS Y FROM athlete_events WHERE age IS NOT NULL AND age &lt;&gt; &#39;NA&#39; AND sport &lt;&gt; &#39;Art Competitions&#39; AND season = &#39;Winter&#39; GROUP BY year) t1) t2 . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . b . -0.13860606401530068 | . We can see a stronger negative trend in summer events than in winter events. . What is the number of participating nations for each year and season? . %%sql result &lt;&lt; SELECT * FROM (SELECT DISTINCT Year, Season, region FROM athlete_events LEFT JOIN noc_regions ON athlete_events.NOC = noc_regions.NOC) t PIVOT ( Count(region) FOR season IN (Summer, Winter) ) piv ORDER BY Year . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. Returning data to local variable result . result_df = result.DataFrame() result_df.plot.bar(x=&#39;Year&#39;, y =[&#39;Summer&#39;, &#39;Winter&#39;], title=&#39;# Participating Nations The Olympics&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;# Participating Nations The Olympics&#39;}, xlabel=&#39;Year&#39;&gt; . Few observation: . The number of participating nations has been increasing since the inception of the first Olympic season in 1896. | The first winter Olympics was held in 1924. | Less nations participate in the winter Olympics as compared with the summer ones. | Since 1992, the Olympics are held every two years for alternating seasons. | . In which years and seasons did Iraq not participate in the Olympics? . %%sql SELECT Year, Season, Sum(Iraq) AS IraqIn FROM (SELECT DISTINCT Year, Season, region, CASE region WHEN &#39;Iraq&#39; THEN 1 ELSE 0 END AS Iraq FROM athlete_events LEFT JOIN noc_regions ON athlete_events.NOC = noc_regions.NOC) t GROUP BY Year, Season HAVING Sum(Iraq) = 0 ORDER BY Year . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . Year Season IraqIn . 1896 | Summer | 0 | . 1900 | Summer | 0 | . 1904 | Summer | 0 | . 1906 | Summer | 0 | . 1908 | Summer | 0 | . 1912 | Summer | 0 | . 1920 | Summer | 0 | . 1924 | Winter | 0 | . 1924 | Summer | 0 | . 1928 | Winter | 0 | . 1928 | Summer | 0 | . 1932 | Winter | 0 | . 1932 | Summer | 0 | . 1936 | Winter | 0 | . 1936 | Summer | 0 | . 1948 | Winter | 0 | . 1952 | Winter | 0 | . 1952 | Summer | 0 | . 1956 | Winter | 0 | . 1956 | Summer | 0 | . 1960 | Winter | 0 | . 1964 | Winter | 0 | . 1968 | Winter | 0 | . 1972 | Summer | 0 | . 1972 | Winter | 0 | . 1976 | Winter | 0 | . 1976 | Summer | 0 | . 1980 | Winter | 0 | . 1984 | Winter | 0 | . 1988 | Winter | 0 | . 1992 | Winter | 0 | . 1994 | Winter | 0 | . 1998 | Winter | 0 | . 2002 | Winter | 0 | . 2006 | Winter | 0 | . 2010 | Winter | 0 | . 2014 | Winter | 0 | . What is the number of medals per Sport for sports which have more than 500 medals granted? . %%sql results &lt;&lt; SELECT Sport, COUNT(Medal) AS MedalsGranted FROM (SELECT DISTINCT CAST(athlete_events.NAME AS CHAR(100)) AS Name, Games, region, Sport, Medal FROM athlete_events LEFT JOIN noc_regions ON athlete_events.NOC = noc_regions.NOC WHERE Medal IS NOT NULL AND Medal &lt;&gt; &#39;NA&#39;) t1 GROUP BY Sport HAVING COUNT(Medal) &gt; 500 ORDER BY MedalsGranted DESC . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. Returning data to local variable results . results_df = results.DataFrame() results_df.plot.bar(x=&#39;Sport&#39;, y=&#39;MedalsGranted&#39;) . &lt;AxesSubplot:xlabel=&#39;Sport&#39;&gt; . Which countries participated less than 5 times in the Olympics? . %%sql SELECT region, Count(Games) AS TimesParticipated FROM (SELECT DISTINCT Games, region FROM athlete_events LEFT JOIN noc_regions ON athlete_events.NOC = noc_regions.NOC WHERE region IS NOT NULL AND region &lt;&gt; &#39;NA&#39;) t1 GROUP BY region HAVING Count(Games) &lt; 5 ORDER BY TimesParticipated . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . region TimesParticipated . Kosovo | 1 | . South Sudan | 1 | . Marshall Islands | 3 | . Kiribati | 4 | . Which are the top 5 countries by the number of Judo players? . %%sql SELECT TOP 5 region, Count(DISTINCT Cast(Name AS NVARCHAR(50))) NumPlayers FROM athlete_events LEFT JOIN noc_regions ON athlete_events.NOC = noc_regions.NOC WHERE Sport = &#39;Judo&#39; GROUP BY region ORDER BY NumPlayers DESC . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . region NumPlayers . Germany | 95 | . South Korea | 95 | . Russia | 94 | . France | 93 | . Japan | 93 | . What is percentage of Judo players in each of these countries? . %%sql result &lt;&lt; SELECT top 20 region, ( CAST(COUNT(DISTINCT CAST(Name AS NVARCHAR(50))) AS FLOAT) / (SELECT COUNT(DISTINCT CAST(Name AS NVARCHAR(50))) NumJudoPlayers FROM athlete_events WHERE sport = &#39;Judo&#39; ) ) * 100 JudoPlayersPercentage FROM athlete_events LEFT JOIN noc_regions ON athlete_events.noc = noc_regions.noc WHERE sport = &#39;Judo&#39; GROUP BY region ORDER BY JudoPlayersPercentage DESC . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. Returning data to local variable result . result_df = result.DataFrame() result_df.plot.bar(x=&#39;region&#39;, y=&#39;JudoPlayersPercentage&#39;) . &lt;AxesSubplot:xlabel=&#39;region&#39;&gt; . Which are the top 5 countries by the number of medals in judo? . %%sql SELECT TOP 5 region, Count(Medal) AS NumMedals FROM (SELECT DISTINCT CAST(NAME AS CHAR(100)) AS Name, Games, region, Medal FROM athlete_events LEFT JOIN noc_regions ON athlete_events.NOC = noc_regions.NOC WHERE sport = &#39;Judo&#39; AND Medal &lt;&gt; &#39;NA&#39;) t1 GROUP BY region ORDER BY NumMedals DESC . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . region NumMedals . Japan | 84 | . France | 49 | . Russia | 43 | . South Korea | 43 | . Germany | 37 | . Who are the top 5 players who participated the largest number of times? . %%sql SELECT TOP 5 CAST(NAME AS CHAR(100)) Name, COUNT(Games) GamesParticipated FROM athlete_events WHERE Sport &lt;&gt; &#39;Art Competitions&#39; GROUP BY CAST(NAME AS CHAR(100)) ORDER BY GamesParticipated DESC . * mssql+pyodbc://@localhost SQLEXPRESS/OlympicsHistory?driver=SQL+Server&amp;trusted_connection=yes Done. . Name GamesParticipated . Heikki Ilmari Savolainen | 39 | . Joseph &quot;Josy&quot; Stoffel | 38 | . Ioannis Theofilakis | 36 | . Takashi Ono | 33 | . Andreas Wecker | 32 | . Conclusions . Writing this article was really interesting, and I enjoyed learning about the Olympics things that I didn&#39;t know. This article is far from being perfect but I feel, it serves its purpose. . Writing SQL statements like this is not has its benefits as well as its limitations. . Postives . The ability to grab the needed data and plot it in Python or apply and subsequent processing to it is very interesting. | . Limitations . If the query returns a large table, Jupyter could freeze and crash. To alleviate this issue, one can limit the size of returned rows by using select top x, for example. | Syntax highlighting, code auto completion, code formatting/linting are missing. | .",
            "url": "https://waseemwaheed.github.io/DataInterrogator/2021/08/28/Efficient-Data-Analysis-SQL-and-Python.html",
            "relUrl": "/2021/08/28/Efficient-Data-Analysis-SQL-and-Python.html",
            "date": " ‚Ä¢ Aug 28, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "A solution to a coding challenge",
            "content": "Introduction . I have recently been invited to a coding challenge which was required to be delivered in 7 days. The task was very simple üòé: . Coding challenge . The aim of this exercise is to implement an ‚Äúalerting‚Äù service which will consume a file of currency conversion rates and produce alerts. . For the purpose of this coding exercise, you are allowed to choose a different programming language, provided that you provide us with a detailed instruction on how to build and run your program. . Input . The format of the file will simulate a stream of currency conversion rates. Each line will be properly structured JSON (http://jsonlines.org/): . { ‚Äútimestamp‚Äù: 1554933784.023, ‚ÄúcurrencyPair‚Äù: ‚ÄúCNYAUD‚Äù, ‚Äúrate‚Äù: 0.39281 } . The fields in the JSON record are: . timestamp: the timestamp of the record in seconds since UNIX epoch, with fractional seconds specified | currencyPair: the sell and buy currencies which the rate relates to | rate: the conversion rate | . You may assume that for each currency pair, currency conversion rates are streamed at a constant rate of one per second. ie. for two consecutive ‚ÄúCNYAUD‚Äù entries in in the input file, they will have timestamps that differ by one second: . { &quot;timestamp&quot;: 1554933784.023, &quot;currencyPair&quot;: &quot;CNYAUD&quot;, &quot;rate&quot;: 0.39281 } { &quot;timestamp&quot;: 1554933784.087, &quot;currencyPair&quot;: &quot;USDAUD&quot;, &quot;rate&quot;: 0.85641 } { &quot;timestamp&quot;: 1554933785.023, &quot;currencyPair&quot;: &quot;CNYAUD&quot;, &quot;rate&quot;: 0.39295 } . Output . The alerting service should produce the following alert as a JSON string output to standard output: . when the spot rate for a currency pair changes by more than 10% from the 5 minute average for that currency pair | . The format of the alert produced should be: . { ‚Äútimestamp‚Äù: 1554933784.023, ‚ÄúcurrencyPair‚Äù: ‚ÄúCNYAUD‚Äù, ‚Äúalert‚Äù: ‚ÄúspotChange‚Äù } . As mentioned earlier, the task is very simple but I wanted to take the opportunity to improve the following aspects: . Code readability | Unit testing | Deployability | . Code readability . I use VSCODE for many tasks, and it is my main text editor so naturally, I looked for tools that play well with it. For automatic code styling, I used black which is a great code formatter. Additionally, I revised the PEP 8 guide to refresh my memory of best practices. . Unit Testing . The idea behind unit testing is that you have to arrange your code into non-coupled components to allow for testing. In the Python ecosystem there are few options such as the unittest which comes as part of the Python Standard Library and pytest. I ended up using pytest because I wanted to learn it. . Deployability . Python, similar to other interpreted languages, requires a compatible version of the interpreter and the same version of packages (excluding Javascript, where every computer nowadays comes with one). This is a common issue that has many solutions. Among those solutions are the virtual environments such as (venv, virtualenv, conda env) and containers such as the well-known docker. . The simplicity and ubiquity of docker made it a simple choice üëç for me in this challenge. Once the code is written and tested, docker image description file is all that is needed. The alternative path of virtual environment was also a viable one, I just had to write enviroment creation scripts, one for windows and one for Unix/Linux ü•±. . My solution . You can access the solution over here . . Assumptions . One input file can be consumed at a time. | The frequency at which updates arrive is fixed (1s). Hence the average, in the general case, is taken over 300 samples. | A single stream (input file) can contain more than one currency pair. | . Decisions . To keep track of the exchange rates for each pair, I made the currency pairs keys of a dictionary which maps to sliding window deque data structure. The rationale behind choosing a dictionary is because, each new line can be new data point for a currency pair, which means the currency pairs data need to be accessed in random order and a dictionary is the best options here where an access operation is $O(1)$ | The rationale behind choosing a deque is that it allows for the easy creation of a sliding window. A deque has $O(1)$ complexity when we append to or access the ends of the queue, which is what we are doing here. | . | The CurrencyPairData class is a subclass of the Observable class. This allows me to easily add callbacks to CurrencyPairData instances. | . In the end . The coding challenge was a fun opportunity to build something and get someone to give me feedback on my approach. Additionally, I had a ton of fun learning about pytest and docker which will definitely be used in my other projects. .",
            "url": "https://waseemwaheed.github.io/DataInterrogator/python/docker/unit%20testing/software%20engineering/data%20structures/json/2021/08/13/Coding-Challenge-Solution.html",
            "relUrl": "/python/docker/unit%20testing/software%20engineering/data%20structures/json/2021/08/13/Coding-Challenge-Solution.html",
            "date": " ‚Ä¢ Aug 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Making the TITAN GTX GPU available to Tensorflow",
            "content": "Introduction . This post is to document üìù a process I had to go through. Installing Tensorflow on Ubuntu. . To make use of the GPU(s) in Tensorflow, Linux should be able to detect the GPU and tensorflow-gpu should be installed. . The process . First of all, you need to check that Linux can detect the GPU as follows: you can either do: . user# sudo lshw -C display . or: . user# nvidia-smi . which should show you a list containing all the NVIDIA GPUs attached to your machine. . Next, to check if tensorflow-gpu is installed, we can use the following command . user# pip list | grep tensor tensorboard 2.4.1 tensorboard-plugin-wit 1.8.0 tensorflow 1.14.0 tensorflow-estimator 2.3.0 . as we can see, tensorflow-gpu is not in the list. To install it, we use: . user# sudo pip install --upgrade pip user# pip install tensorflow-gpu --user . once finished, we should check the correct operation of Tensorflow as follows: . user# python . import tensorflow as tf print(tf.config.experimental.list_physical_devices(&#39;GPU&#39;)) . which should give you a list of the installed GPUs. To use a specific GPU within a context: . tf.debugging.set_log_device_placement(True) try: # Specify an invalid GPU device with tf.device(&#39;/device:GPU:1&#39;): a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]) b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) c = tf.matmul(a, b) except RuntimeError as e: print(e) . To use a specific GPU for all Tensorflow‚Äôs calculations, use the following template: . import tensorflow as tf gpus = tf.config.experimental.list_physical_devices(&#39;GPU&#39;) if gpus: # Restrict TensorFlow to only use the first GPU try: tf.config.experimental.set_visible_devices(gpus[0], &#39;GPU&#39;) logical_gpus = tf.config.experimental.list_logical_devices(&#39;GPU&#39;) print(len(gpus), &quot;Physical GPUs,&quot;, len(logical_gpus), &quot;Logical GPU&quot;) except RuntimeError as e: # Visible devices must be set before GPUs have been initialized print(e) . Note: this note took advantage of Tensorflow‚Äôs documentation @ https://www.tensorflow.org/guide/gpu . In the end . I hope that this write up has helped you installing from scratch/fixing your Tensorflow installation. .",
            "url": "https://waseemwaheed.github.io/DataInterrogator/python/linux/tensorflow/software%20engineering/2020/11/15/Making-TITAN-GTX-GPU-available-to-Tensorflow.html",
            "relUrl": "/python/linux/tensorflow/software%20engineering/2020/11/15/Making-TITAN-GTX-GPU-available-to-Tensorflow.html",
            "date": " ‚Ä¢ Nov 15, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "ID Mapping using Selenium",
            "content": "At work, we have to keep track of the progress of students where each student is identified by their ID (matching by the name is a tricky thing and not reliable). . The problem . Instead of a single ID, students have two! . You may say, that should be fine, you must have a database mapping those IDs to each other isn‚Äôt it? and the answer is yes but, there is a BUT, I can access one student at a time through an online portal, which is kinda fun, you know üôÑ. . The solution . But, then you may say, hang on, the DBA should be able to provide you with the data you need, isn‚Äôt it. To you I say, I wish that was the case, anyways. . To me this is the sort of thing that I like to spend my free time on üòé, so I spent an hour or so, figuring how to do it üê±‚Äçüíª. I knew that there is a nice web browser automation tool called Selenium, but I had never used at that point. . With a bit of Pandas, I was able to get job done and I am very happy with the result, the script is accessible here. .",
            "url": "https://waseemwaheed.github.io/DataInterrogator/python/selenium/pandas/2019/08/15/ID-Mapping-using-Selenium.html",
            "relUrl": "/python/selenium/pandas/2019/08/15/ID-Mapping-using-Selenium.html",
            "date": " ‚Ä¢ Aug 15, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Benford's Law",
            "content": "Benford‚Äôs law . Few days ago, I was introduced to the Benford‚Äôs law of distribution. Looking up the internet for accessible explanation, I came across a basic introduction with very interesting explanation on Khan Academy. Sal mentioned the typical example of the world countries populations follow this fascinating distribution function. . After few seconds of thinking about it, I decided to confirm this claim, So, I quickly pulled the data from wikipedia and coded a quick and dirty experiment. And sure it works as you can see bellow . . Code and data can be found in my GitHub .",
            "url": "https://waseemwaheed.github.io/DataInterrogator/analysis/probability/2017/07/07/benfords-law.html",
            "relUrl": "/analysis/probability/2017/07/07/benfords-law.html",
            "date": " ‚Ä¢ Jul 7, 2017"
        }
        
    
  
    
        ,"post5": {
            "title": "D3 - visualizing temperature 2",
            "content": "Today‚Äôs post is a bit delayed, anyway, I went through the same data from day 1 (temperature measurements) but this time I, used leaflet.js instead of google maps API starting from from the example. The beautiful thing about leaflet is that it doesn‚Äôt abstract the whole DOM event-handling like in google maps which means events set using d3 are handled normally. . I have manged to encoded the data in color, size and in text in the form of tool-tips. . . The code can be found at here .",
            "url": "https://waseemwaheed.github.io/DataInterrogator/d3/visualization/2017/07/03/D3js-Explorations-4.html",
            "relUrl": "/d3/visualization/2017/07/03/D3js-Explorations-4.html",
            "date": " ‚Ä¢ Jul 3, 2017"
        }
        
    
  
    
        ,"post6": {
            "title": "D3 - visualizing connections",
            "content": "Today work is my own go at arc diagrams replicating a worked example at Matthew Clemens‚Äôs blog post. My code can be found at Github . .",
            "url": "https://waseemwaheed.github.io/DataInterrogator/d3/visualization/2017/07/01/D3js-Explorations-3.html",
            "relUrl": "/d3/visualization/2017/07/01/D3js-Explorations-3.html",
            "date": " ‚Ä¢ Jul 1, 2017"
        }
        
    
  
    
        ,"post7": {
            "title": "D3 - Visualizing bike share stations on map",
            "content": "I started today with a goal to plot data from Melbourne city particularly Melbourne Bike Share stations on google maps with marker size and color representing the size of the station and the availability of bikes respectively in addition to tooltips containing the details of each station on hover. . Mapping the data went smoothly but tooltips turned to be tricky, it took me some time to decide to postpone it to another stage. I think the problem has to do with google maps handling events not D3 so in other words events are not being passed to D3. Handling events with google maps is straight forward but finding the current location of the markers to compare with the current mouse position is not clear at this point. . Anyway, I think the visualization is still interesting. . .",
            "url": "https://waseemwaheed.github.io/DataInterrogator/d3/visualization/2017/06/30/d3js-Explorations-2.html",
            "relUrl": "/d3/visualization/2017/06/30/d3js-Explorations-2.html",
            "date": " ‚Ä¢ Jun 30, 2017"
        }
        
    
  
    
        ,"post8": {
            "title": "D3 - Visualizing temperature 1",
            "content": "This is the first day of the challenge but I have been tinkering with D3 for few days now. . My first day started with an intention to project points on a map so, I had a look at few examples by they seemed complicated so I adjusted my aims from drawing the map with d3 to mapping the data with d3 and get the map from a map provider (google maps or leaflet). I started with an example by Mike Bostock which is very accessible and I adapted it. My data that I had which is the locations of temperature sensors spread across the US (source : GSOD) as well as summary temperature measurement for one day. I‚Äôm happy with the result for today as I managed to project the sensors on the map and color coded the points based on temperature measurement. . My adapted version of the code can be found on my github . . Future directions : . Draw connections between nodes | include a slider to select the day of the year. |",
            "url": "https://waseemwaheed.github.io/DataInterrogator/d3/visualization/2017/06/29/D3js-Explorations-1.html",
            "relUrl": "/d3/visualization/2017/06/29/D3js-Explorations-1.html",
            "date": " ‚Ä¢ Jun 29, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi this is me . My main skills are: . Software engineering (Python, Java, C) | Predictive analytics (R, Python) | Data analysis (SQL, R, Python, Excel) | Data visualization (PowerBI, matplotlib, seaborn, ggplot) | Technical writing | . I have a PhD in Image processing, and my main research area is continuous optimization techniques in signal and image processing. . My thesis is at the intersection of: . Optimization | Signal processing | Image processing | Graph theory | I currently work as a sessional lecturer in the computer science department at both La Trobe University and Charles Sturt Univesity. The subjects I teach include: Natural Language Processing, Internet Of Things, Wireless Communications/Networking. . In 2018, I spent 5 months at Aurecon working on three data science projects in the area of asset management, mainly developing image processing/computer vision and machine learning solutions targeted at streamlining asset management processes. . I am broadly interested in data science, computer vision, natural language processing, computer programming, computer networking and IT in general. .",
          "url": "https://waseemwaheed.github.io/DataInterrogator/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://waseemwaheed.github.io/DataInterrogator/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}